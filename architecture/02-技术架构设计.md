# IntelliKnowledge-RAG 技术架构设计

## 1. 整体架构

### 1.1 系统架构图

```
┌─────────────────────────────────────────────────────────────────────┐
│                           Frontend Layer                             │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────────────────┐  │
│  │   React UI   │  │  管理控制台  │  │   移动端 (可选)          │  │
│  └──────┬───────┘  └──────┬───────┘  └────────┬─────────────────┘  │
└─────────┼──────────────────┼──────────────────┼────────────────────┘
          │                  │                  │
          └──────────────────┴──────────────────┘
                             │ HTTPS/WebSocket
┌─────────────────────────────┼────────────────────────────────────────┐
│                    API Gateway (Nginx/Kong)                          │
│              Load Balancer + Rate Limiting + Auth                    │
└─────────────────────────────┬────────────────────────────────────────┘
                              │
┌─────────────────────────────┴────────────────────────────────────────┐
│                        Application Layer                             │
│  ┌──────────────────────────────────────────────────────────────┐   │
│  │                     FastAPI Backend                          │   │
│  │  ┌────────────┐  ┌────────────┐  ┌────────────────────────┐│   │
│  │  │  Ingestion │  │ Retrieval  │  │   Generation Service   ││   │
│  │  │   Service  │  │  Service   │  │   (Agentic Workflow)   ││   │
│  │  └─────┬──────┘  └─────┬──────┘  └──────────┬─────────────┘│   │
│  │        │                │                    │               │   │
│  │  ┌─────▼────────────────▼────────────────────▼─────────────┐│   │
│  │  │              LangGraph Agent Orchestrator                ││   │
│  │  │  ┌─────────┐ ┌──────────┐ ┌─────────────────────────┐  ││   │
│  │  │  │Planning │ │Retrieval │ │ Reflection Agent        │  ││   │
│  │  │  │ Agent   │ │ Agent    │ │ (Self-Critique)         │  ││   │
│  │  │  └─────────┘ └──────────┘ └─────────────────────────┘  ││   │
│  │  └───────────────────────────────────────────────────────┘│   │
│  └──────────────────────────────────────────────────────────────┘   │
└──────────────────────────┬───────────────────────────────────────────┘
                           │
┌──────────────────────────┴───────────────────────────────────────────┐
│                         Storage Layer                                │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐  ┌───────────────┐ │
│  │  Vector DB │  │PostgreSQL  │  │   Neo4j    │  │  Object Store │ │
│  │   Milvus   │  │ (Metadata) │  │ (Knowledge │  │   (MinIO/S3)  │ │
│  │            │  │            │  │   Graph)   │  │               │ │
│  └────────────┘  └────────────┘  └────────────┘  └───────────────┘ │
└──────────────────────────────────────────────────────────────────────┘
                           │
┌──────────────────────────┴───────────────────────────────────────────┐
│                    Infrastructure Layer                              │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐  ┌───────────────┐ │
│  │   Redis    │  │  RabbitMQ  │  │ Prometheus │  │  ELK Stack    │ │
│  │  (Cache)   │  │  (Queue)   │  │ + Grafana  │  │  (Logging)    │ │
│  └────────────┘  └────────────┘  └────────────┘  └───────────────┘ │
└──────────────────────────────────────────────────────────────────────┘
```

---

## 2. 核心模块设计

### 2.1 文档摄入服务 (Ingestion Service)

#### 2.1.1 处理流程

```python
# 文档摄入Pipeline
class IngestionPipeline:
    def process(self, file: UploadFile) -> Document:
        # 1. 文件检测与验证
        file_type = detect_file_type(file)
        validate_file(file, max_size=100MB)

        # 2. 选择解析器
        parser = ParserFactory.create(file_type)
        raw_content = parser.parse(file)

        # 3. 内容清洗
        cleaned_content = ContentCleaner.clean(raw_content)

        # 4. 文档分块 (Chunking)
        chunks = ChunkingStrategy.split(
            content=cleaned_content,
            strategy="semantic",  # semantic / fixed / recursive
            chunk_size=512,
            chunk_overlap=50
        )

        # 5. Metadata提取
        metadata = MetadataExtractor.extract(file, chunks)

        # 6. Embedding生成 (批量处理)
        embeddings = EmbeddingService.batch_embed(
            texts=[chunk.text for chunk in chunks],
            model="text-embedding-3-large"
        )

        # 7. 存储
        doc_id = self._store_document(chunks, embeddings, metadata)

        # 8. 异步任务：知识图谱构建
        task_queue.publish(KnowledgeGraphTask(doc_id))

        return Document(id=doc_id, status="completed")
```

#### 2.1.2 文档解析器

```python
# 解析器接口
class BaseParser(ABC):
    @abstractmethod
    def parse(self, file: UploadFile) -> RawContent:
        pass

# PDF解析器
class PDFParser(BaseParser):
    def parse(self, file: UploadFile) -> RawContent:
        # 使用 PyMuPDF 提取文本、图片、表格
        doc = fitz.open(stream=file.read(), filetype="pdf")

        content = RawContent()
        for page_num, page in enumerate(doc):
            # 提取文本
            text = page.get_text("text")
            content.add_text(text, page=page_num)

            # 提取图片
            images = page.get_images()
            for img in images:
                content.add_image(extract_image(img), page=page_num)

            # 提取表格（使用Camelot或LLM识别）
            tables = TableExtractor.extract(page)
            content.add_tables(tables, page=page_num)

        return content

# Markdown解析器
class MarkdownParser(BaseParser):
    def parse(self, file: UploadFile) -> RawContent:
        text = file.read().decode('utf-8')
        # 解析frontmatter
        metadata, content = parse_frontmatter(text)

        # 提取标题结构
        headers = extract_headers(content)

        return RawContent(
            text=content,
            metadata=metadata,
            structure=headers
        )
```

#### 2.1.3 智能分块策略

```python
class SemanticChunker:
    """基于语义边界的分块"""
    def __init__(self, embedding_model):
        self.model = embedding_model

    def split(self, text: str, chunk_size: int = 512) -> List[Chunk]:
        # 1. 先按句子分割
        sentences = self._split_sentences(text)

        # 2. 计算句子embeddings
        embeddings = self.model.embed(sentences)

        # 3. 计算相邻句子的相似度
        similarities = cosine_similarity(embeddings[:-1], embeddings[1:])

        # 4. 在相似度低的地方切分
        split_points = np.where(similarities < threshold)[0]

        # 5. 合并小块，确保每块大小合适
        chunks = self._merge_chunks(sentences, split_points, chunk_size)

        return chunks
```

---

### 2.2 检索服务 (Retrieval Service)

#### 2.2.1 混合检索架构

```python
class HybridRetriever:
    def __init__(self):
        self.vector_retriever = MilvusRetriever()
        self.keyword_retriever = BM25Retriever()
        self.graph_retriever = Neo4jRetriever()
        self.reranker = CrossEncoderReranker()

    async def retrieve(
        self,
        query: str,
        top_k: int = 10,
        filters: Dict = None
    ) -> List[Document]:

        # 并行执行多个检索
        results = await asyncio.gather(
            self.vector_retriever.search(query, top_k=20, filters=filters),
            self.keyword_retriever.search(query, top_k=20),
            self.graph_retriever.search(query, top_k=10)
        )

        # 融合结果 (RRF算法)
        merged = self._reciprocal_rank_fusion(
            results,
            weights=[0.5, 0.3, 0.2]  # 向量、关键词、图谱权重
        )

        # Rerank
        reranked = self.reranker.rerank(
            query=query,
            documents=merged[:50],
            top_k=top_k
        )

        return reranked

    def _reciprocal_rank_fusion(
        self,
        results: List[List[Document]],
        weights: List[float],
        k: int = 60
    ) -> List[Document]:
        """倒数排名融合算法"""
        scores = defaultdict(float)

        for result_list, weight in zip(results, weights):
            for rank, doc in enumerate(result_list):
                scores[doc.id] += weight / (k + rank + 1)

        # 按分数排序
        sorted_docs = sorted(
            scores.items(),
            key=lambda x: x[1],
            reverse=True
        )

        return [get_document(doc_id) for doc_id, _ in sorted_docs]
```

#### 2.2.2 向量检索优化

```python
# Milvus配置
collection_schema = {
    "fields": [
        {"name": "id", "type": "INT64", "is_primary": True},
        {"name": "embedding", "type": "FLOAT_VECTOR", "dim": 1536},
        {"name": "text", "type": "VARCHAR", "max_length": 2000},
        {"name": "document_id", "type": "INT64"},
        {"name": "metadata", "type": "JSON"}
    ]
}

# 索引配置
index_params = {
    "metric_type": "COSINE",
    "index_type": "HNSW",  # 高性能图索引
    "params": {
        "M": 16,  # 每个节点的邻居数
        "efConstruction": 200
    }
}

# 查询优化
search_params = {
    "metric_type": "COSINE",
    "params": {
        "ef": 100,  # 搜索时的候选集大小
        "nprobe": 10  # 搜索的聚类数
    }
}
```

---

### 2.3 Agentic RAG 编排

#### 2.3.1 LangGraph工作流

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated

class AgentState(TypedDict):
    query: str
    sub_queries: List[str]
    retrieved_docs: List[Document]
    answer: str
    confidence: float
    iteration: int

# 定义Agent节点
def planning_node(state: AgentState) -> AgentState:
    """规划节点：分解复杂查询"""
    llm = ChatOpenAI(model="gpt-4-turbo")

    prompt = f"""
    用户查询：{state['query']}

    请判断这个查询是否需要分解为多个子问题。
    如果需要，列出子问题；如果不需要，返回原查询。
    """

    response = llm.invoke(prompt)
    sub_queries = parse_sub_queries(response)

    return {**state, "sub_queries": sub_queries}

def retrieval_node(state: AgentState) -> AgentState:
    """检索节点"""
    retriever = HybridRetriever()

    all_docs = []
    for query in state['sub_queries']:
        docs = retriever.retrieve(query, top_k=5)
        all_docs.extend(docs)

    # 去重
    unique_docs = deduplicate_documents(all_docs)

    return {**state, "retrieved_docs": unique_docs}

def generation_node(state: AgentState) -> AgentState:
    """生成节点"""
    llm = ChatOpenAI(model="gpt-4-turbo")

    context = "\n\n".join([
        f"[文档{i+1}] {doc.content}"
        for i, doc in enumerate(state['retrieved_docs'])
    ])

    prompt = f"""
    基于以下检索到的文档回答问题：

    {context}

    问题：{state['query']}

    要求：
    1. 仅使用文档中的信息
    2. 标注引用来源
    3. 如果文档中没有相关信息，明确说明
    """

    answer = llm.invoke(prompt)

    return {**state, "answer": answer.content}

def reflection_node(state: AgentState) -> AgentState:
    """反思节点：评估答案质量"""
    llm = ChatOpenAI(model="gpt-4-turbo")

    prompt = f"""
    问题：{state['query']}
    答案：{state['answer']}

    请评估这个答案的质量（0-1分）：
    - 是否完整回答了问题
    - 是否有逻辑漏洞
    - 是否需要补充信息

    返回JSON格式：{{"confidence": 0.8, "issues": ["..."]}}
    """

    evaluation = llm.invoke(prompt)
    result = json.loads(evaluation.content)

    return {
        **state,
        "confidence": result["confidence"],
        "iteration": state.get("iteration", 0) + 1
    }

def should_continue(state: AgentState) -> str:
    """决策路由"""
    if state["confidence"] > 0.8:
        return "end"
    elif state.get("iteration", 0) >= 3:
        return "end"
    else:
        return "retrieve"  # 重新检索

# 构建工作流图
workflow = StateGraph(AgentState)

workflow.add_node("planning", planning_node)
workflow.add_node("retrieval", retrieval_node)
workflow.add_node("generation", generation_node)
workflow.add_node("reflection", reflection_node)

workflow.set_entry_point("planning")
workflow.add_edge("planning", "retrieval")
workflow.add_edge("retrieval", "generation")
workflow.add_edge("generation", "reflection")

workflow.add_conditional_edges(
    "reflection",
    should_continue,
    {
        "retrieve": "retrieval",
        "end": END
    }
)

agent = workflow.compile()
```

#### 2.3.2 查询改写

```python
class QueryRewriter:
    def __init__(self, llm):
        self.llm = llm

    def rewrite(self, query: str, context: List[str] = None) -> str:
        """
        改写策略：
        1. 补全模糊查询
        2. 纠正错别字
        3. 扩展同义词
        4. 添加时间/领域限定
        """
        prompt = f"""
        原始查询：{query}

        请改写这个查询，使其更适合语义检索：
        - 补全缺失的上下文
        - 使用更精确的术语
        - 添加必要的限定词

        只返回改写后的查询，不要解释。
        """

        if context:
            prompt += f"\n\n对话历史：\n" + "\n".join(context[-3:])

        rewritten = self.llm.invoke(prompt)
        return rewritten.content.strip()
```

---

### 2.4 知识图谱模块

#### 2.4.1 实体关系抽取

```python
class KnowledgeGraphBuilder:
    def __init__(self):
        self.ner_model = spacy.load("en_core_web_lg")
        self.relation_extractor = LLMRelationExtractor()
        self.graph_db = Neo4jClient()

    async def build_graph(self, document: Document):
        # 1. 实体识别
        entities = self._extract_entities(document.text)

        # 2. 关系抽取
        relations = await self._extract_relations(document.text, entities)

        # 3. 存储到Neo4j
        await self._store_graph(entities, relations, document.id)

    def _extract_entities(self, text: str) -> List[Entity]:
        doc = self.ner_model(text)
        entities = []

        for ent in doc.ents:
            entities.append(Entity(
                text=ent.text,
                type=ent.label_,
                start=ent.start_char,
                end=ent.end_char
            ))

        return entities

    async def _extract_relations(
        self,
        text: str,
        entities: List[Entity]
    ) -> List[Relation]:
        # 使用LLM提取关系
        prompt = f"""
        文本：{text}

        实体列表：{[e.text for e in entities]}

        请提取实体之间的关系，返回JSON格式：
        [
            {{"subject": "实体1", "predicate": "关系", "object": "实体2"}},
            ...
        ]
        """

        response = await self.relation_extractor.extract(prompt)
        return parse_relations(response)

    async def _store_graph(
        self,
        entities: List[Entity],
        relations: List[Relation],
        doc_id: str
    ):
        # Cypher查询：创建节点和关系
        for entity in entities:
            await self.graph_db.execute(f"""
                MERGE (e:Entity {{name: $name, type: $type}})
                ON CREATE SET e.first_seen = timestamp()
                ON MATCH SET e.last_seen = timestamp()
                """,
                name=entity.text,
                type=entity.type
            )

        for rel in relations:
            await self.graph_db.execute(f"""
                MATCH (s:Entity {{name: $subject}})
                MATCH (o:Entity {{name: $object}})
                MERGE (s)-[r:{rel.predicate}]->(o)
                SET r.source_doc = $doc_id
                """,
                subject=rel.subject,
                object=rel.object,
                doc_id=doc_id
            )
```

#### 2.4.2 图检索

```python
class GraphRetriever:
    def __init__(self, neo4j_client):
        self.db = neo4j_client

    async def search(self, query: str, top_k: int = 10) -> List[Document]:
        # 1. 从查询中提取实体
        entities = extract_entities(query)

        # 2. 在图中查找相关子图
        subgraph = await self._find_subgraph(entities, max_hops=2)

        # 3. 获取相关文档
        docs = await self._get_documents_from_graph(subgraph)

        return docs[:top_k]

    async def _find_subgraph(
        self,
        entities: List[str],
        max_hops: int = 2
    ) -> List[Path]:
        # Cypher: 查找多跳路径
        query = """
        MATCH path = (start:Entity)-[*1..{max_hops}]-(end:Entity)
        WHERE start.name IN $entities
        RETURN path
        ORDER BY length(path) ASC
        LIMIT 100
        """.format(max_hops=max_hops)

        result = await self.db.execute(query, entities=entities)
        return result
```

---

## 3. 数据模型设计

### 3.1 PostgreSQL Schema

```sql
-- 文档表
CREATE TABLE documents (
    id BIGSERIAL PRIMARY KEY,
    title VARCHAR(500),
    file_name VARCHAR(255),
    file_type VARCHAR(50),
    file_size BIGINT,
    file_hash VARCHAR(64) UNIQUE,  -- 用于去重
    user_id BIGINT,
    collection_id BIGINT,
    status VARCHAR(20),  -- 'processing', 'completed', 'failed'
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),

    -- Metadata (JSONB支持索引)
    metadata JSONB,

    CONSTRAINT fk_user FOREIGN KEY (user_id) REFERENCES users(id),
    CONSTRAINT fk_collection FOREIGN KEY (collection_id) REFERENCES collections(id)
);

CREATE INDEX idx_documents_user ON documents(user_id);
CREATE INDEX idx_documents_metadata ON documents USING GIN(metadata);

-- 文档块表
CREATE TABLE chunks (
    id BIGSERIAL PRIMARY KEY,
    document_id BIGINT NOT NULL,
    chunk_index INT,
    text TEXT NOT NULL,
    token_count INT,

    -- 嵌入向量ID (存储在Milvus中)
    embedding_id VARCHAR(100),

    -- 元数据
    page_number INT,
    section_title VARCHAR(500),
    metadata JSONB,

    created_at TIMESTAMP DEFAULT NOW(),

    CONSTRAINT fk_document FOREIGN KEY (document_id) REFERENCES documents(id) ON DELETE CASCADE
);

CREATE INDEX idx_chunks_document ON chunks(document_id);
CREATE INDEX idx_chunks_embedding ON chunks(embedding_id);

-- 知识库集合
CREATE TABLE collections (
    id BIGSERIAL PRIMARY KEY,
    name VARCHAR(200) NOT NULL,
    description TEXT,
    user_id BIGINT,
    is_public BOOLEAN DEFAULT FALSE,

    -- 配置
    settings JSONB,

    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- 对话历史
CREATE TABLE conversations (
    id BIGSERIAL PRIMARY KEY,
    user_id BIGINT,
    collection_id BIGINT,
    title VARCHAR(500),
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE messages (
    id BIGSERIAL PRIMARY KEY,
    conversation_id BIGINT NOT NULL,
    role VARCHAR(20),  -- 'user', 'assistant'
    content TEXT,

    -- 检索上下文
    retrieved_chunks JSONB,  -- 引用的chunk IDs
    metadata JSONB,

    created_at TIMESTAMP DEFAULT NOW(),

    CONSTRAINT fk_conversation FOREIGN KEY (conversation_id) REFERENCES conversations(id) ON DELETE CASCADE
);

-- 用户表
CREATE TABLE users (
    id BIGSERIAL PRIMARY KEY,
    username VARCHAR(100) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255),
    role VARCHAR(20) DEFAULT 'user',  -- 'user', 'admin'

    -- API配置
    api_keys JSONB,

    created_at TIMESTAMP DEFAULT NOW(),
    last_login TIMESTAMP
);
```

### 3.2 Milvus Collection Schema

```python
# 向量集合定义
collection_schema = CollectionSchema(
    fields=[
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name="chunk_id", dtype=DataType.INT64),
        FieldSchema(name="document_id", dtype=DataType.INT64),
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=1536),
        FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=5000),
        FieldSchema(name="metadata", dtype=DataType.JSON)
    ],
    description="Document chunks with embeddings"
)

# 创建索引
index_params = {
    "metric_type": "COSINE",
    "index_type": "HNSW",
    "params": {"M": 16, "efConstruction": 200}
}
collection.create_index(field_name="embedding", index_params=index_params)

# 分区策略（按collection_id分区）
collection.create_partition("collection_1")
collection.create_partition("collection_2")
```

### 3.3 Neo4j图模型

```cypher
// 节点类型
(:Entity {name: string, type: string, description: string})
(:Document {id: int, title: string, source: string})
(:Concept {name: string, category: string})

// 关系类型
(:Entity)-[:RELATED_TO {weight: float}]->(:Entity)
(:Entity)-[:MENTIONED_IN {position: int}]->(:Document)
(:Concept)-[:IS_A]->(:Concept)
(:Entity)-[:HAS_ATTRIBUTE {value: string}]->(:Entity)

// 示例数据
CREATE (e1:Entity {name: "LangChain", type: "Technology"})
CREATE (e2:Entity {name: "LangGraph", type: "Technology"})
CREATE (e3:Entity {name: "RAG", type: "Concept"})
CREATE (e1)-[:RELATED_TO {weight: 0.9}]->(e2)
CREATE (e1)-[:USED_FOR]->(e3)
```

---

## 4. API设计

### 4.1 RESTful API

```python
# FastAPI路由定义
from fastapi import FastAPI, UploadFile, Depends
from fastapi.responses import StreamingResponse

app = FastAPI(title="IntelliKnowledge API", version="1.0")

# 文档管理
@app.post("/api/v1/documents/upload")
async def upload_document(
    file: UploadFile,
    collection_id: int,
    user: User = Depends(get_current_user)
):
    """上传文档"""
    pass

@app.get("/api/v1/documents/{doc_id}")
async def get_document(doc_id: int):
    """获取文档详情"""
    pass

@app.delete("/api/v1/documents/{doc_id}")
async def delete_document(doc_id: int):
    """删除文档"""
    pass

# 检索
@app.post("/api/v1/search")
async def search(request: SearchRequest):
    """
    请求体：
    {
        "query": "什么是RAG?",
        "collection_id": 1,
        "top_k": 5,
        "filters": {"file_type": "pdf"}
    }
    """
    pass

# 问答（流式）
@app.post("/api/v1/chat")
async def chat(request: ChatRequest):
    """
    请求体：
    {
        "query": "解释一下LangGraph的工作原理",
        "conversation_id": 123,
        "stream": true
    }
    """
    async def generate():
        async for chunk in agent.astream(request.query):
            yield f"data: {json.dumps(chunk)}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")

# 配置管理
@app.put("/api/v1/settings/llm")
async def update_llm_settings(settings: LLMSettings):
    """更新LLM配置"""
    pass

@app.get("/api/v1/settings")
async def get_settings():
    """获取当前配置"""
    pass

# 知识图谱
@app.get("/api/v1/graph/entities/{entity_name}")
async def get_entity_relations(entity_name: str):
    """获取实体关系"""
    pass

@app.get("/api/v1/graph/subgraph")
async def get_subgraph(entities: List[str], max_hops: int = 2):
    """获取子图"""
    pass
```

---

## 5. 性能优化策略

### 5.1 缓存策略

```python
# 多层缓存架构
class CacheManager:
    def __init__(self):
        self.l1_cache = LRUCache(maxsize=1000)  # 内存缓存
        self.l2_cache = RedisCache()  # 分布式缓存

    async def get_or_compute(
        self,
        key: str,
        compute_fn: Callable,
        ttl: int = 3600
    ):
        # L1缓存
        if key in self.l1_cache:
            return self.l1_cache[key]

        # L2缓存
        cached = await self.l2_cache.get(key)
        if cached:
            self.l1_cache[key] = cached
            return cached

        # 计算结果
        result = await compute_fn()

        # 写入缓存
        self.l1_cache[key] = result
        await self.l2_cache.set(key, result, ttl=ttl)

        return result

# 使用示例
cache = CacheManager()

async def get_embedding(text: str):
    cache_key = f"emb:{hash(text)}"
    return await cache.get_or_compute(
        key=cache_key,
        compute_fn=lambda: embedding_model.embed(text),
        ttl=86400  # 24小时
    )
```

### 5.2 异步处理

```python
# 任务队列
from celery import Celery

celery_app = Celery('intelliknowledge', broker='pyamqp://guest@localhost//')

@celery_app.task
def process_document_async(doc_id: int):
    """异步处理文档"""
    pipeline = IngestionPipeline()
    pipeline.process(doc_id)

@celery_app.task
def build_knowledge_graph(doc_id: int):
    """异步构建知识图谱"""
    builder = KnowledgeGraphBuilder()
    builder.build_graph(doc_id)

# 调用
@app.post("/api/v1/documents/upload")
async def upload_document(file: UploadFile):
    # 同步保存文件
    doc_id = save_file(file)

    # 异步处理
    process_document_async.delay(doc_id)

    return {"doc_id": doc_id, "status": "processing"}
```

### 5.3 批处理优化

```python
class BatchEmbeddingService:
    def __init__(self, batch_size=32):
        self.batch_size = batch_size
        self.queue = []
        self.lock = asyncio.Lock()

    async def embed(self, text: str) -> np.ndarray:
        async with self.lock:
            self.queue.append(text)

            # 达到batch_size或超时时处理
            if len(self.queue) >= self.batch_size:
                return await self._process_batch()
            else:
                # 等待更多请求
                await asyncio.sleep(0.1)
                return await self._process_batch()

    async def _process_batch(self):
        if not self.queue:
            return None

        batch = self.queue[:self.batch_size]
        self.queue = self.queue[self.batch_size:]

        # 批量调用embedding API
        embeddings = await embedding_model.batch_embed(batch)

        return embeddings
```

---

## 6. 部署架构

### 6.1 Docker Compose部署

```yaml
# docker-compose.yml
version: '3.8'

services:
  # 后端服务
  api:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@postgres:5432/intelliknowledge
      - REDIS_URL=redis://redis:6379
      - MILVUS_HOST=milvus-standalone
    depends_on:
      - postgres
      - redis
      - milvus-standalone
    volumes:
      - ./uploads:/app/uploads

  # 前端服务
  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    environment:
      - REACT_APP_API_URL=http://localhost:8000

  # PostgreSQL
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: intelliknowledge
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # Redis
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  # Milvus
  milvus-standalone:
    image: milvusdb/milvus:v2.3.0
    ports:
      - "19530:19530"
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
    depends_on:
      - etcd
      - minio

  # Neo4j (可选)
  neo4j:
    image: neo4j:5.12
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      NEO4J_AUTH: neo4j/password
    volumes:
      - neo4j_data:/data

  # RabbitMQ
  rabbitmq:
    image: rabbitmq:3-management
    ports:
      - "5672:5672"
      - "15672:15672"

  # Celery Worker
  celery-worker:
    build: ./backend
    command: celery -A app.celery worker --loglevel=info
    depends_on:
      - rabbitmq
      - postgres

volumes:
  postgres_data:
  neo4j_data:
```

### 6.2 Kubernetes部署（生产环境）

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: intelliknowledge-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: intelliknowledge-api
  template:
    metadata:
      labels:
        app: intelliknowledge-api
    spec:
      containers:
      - name: api
        image: intelliknowledge/api:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: intelliknowledge-api
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

---

## 7. 监控与可观测性

### 7.1 指标收集

```python
from prometheus_client import Counter, Histogram, Gauge

# 定义指标
request_count = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint'])
request_duration = Histogram('http_request_duration_seconds', 'HTTP request latency')
embedding_cost = Counter('embedding_tokens_total', 'Total embedding tokens')
llm_cost = Counter('llm_tokens_total', 'Total LLM tokens', ['model'])

# 中间件
@app.middleware("http")
async def monitor_requests(request: Request, call_next):
    start_time = time.time()

    response = await call_next(request)

    duration = time.time() - start_time
    request_count.labels(method=request.method, endpoint=request.url.path).inc()
    request_duration.observe(duration)

    return response

# 暴露指标端点
@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
```

### 7.2 分布式追踪

```python
from opentelemetry import trace
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

tracer = trace.get_tracer(__name__)

@app.post("/api/v1/chat")
async def chat(request: ChatRequest):
    with tracer.start_as_current_span("chat_request") as span:
        span.set_attribute("query", request.query)

        with tracer.start_as_current_span("retrieval"):
            docs = await retriever.retrieve(request.query)
            span.set_attribute("retrieved_docs", len(docs))

        with tracer.start_as_current_span("generation"):
            answer = await generator.generate(request.query, docs)

        return answer
```

---

## 8. 安全设计

### 8.1 认证授权

```python
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from jose import JWTError, jwt

security = HTTPBearer()

async def get_current_user(
    credentials: HTTPAuthorizationCredentials = Depends(security)
) -> User:
    token = credentials.credentials
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=["HS256"])
        user_id = payload.get("sub")
        if user_id is None:
            raise HTTPException(status_code=401)

        user = await get_user_by_id(user_id)
        return user
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")

# 权限检查
def require_permission(permission: str):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, user: User = Depends(get_current_user), **kwargs):
            if not user.has_permission(permission):
                raise HTTPException(status_code=403)
            return await func(*args, user=user, **kwargs)
        return wrapper
    return decorator

@app.delete("/api/v1/documents/{doc_id}")
@require_permission("documents:delete")
async def delete_document(doc_id: int, user: User = Depends(get_current_user)):
    pass
```

### 8.2 数据加密

```python
from cryptography.fernet import Fernet

class EncryptionService:
    def __init__(self, key: bytes):
        self.cipher = Fernet(key)

    def encrypt(self, data: str) -> str:
        return self.cipher.encrypt(data.encode()).decode()

    def decrypt(self, encrypted: str) -> str:
        return self.cipher.decrypt(encrypted.encode()).decode()

# 敏感字段加密
class UserModel:
    def save_api_key(self, api_key: str):
        encrypted = encryption_service.encrypt(api_key)
        self.encrypted_api_key = encrypted
```

---

## 9. 技术栈总结

| 层级 | 技术选型 | 说明 |
|------|----------|------|
| **前端** | React 18 + TypeScript | 类型安全 |
| | Ant Design / shadcn/ui | UI组件库 |
| | TanStack Query | 数据请求 |
| | Zustand | 状态管理 |
| **后端** | FastAPI | 高性能异步框架 |
| | LangGraph | Agent编排 |
| | LangChain | RAG工具链 |
| **向量数据库** | Milvus | 企业级向量搜索 |
| **关系数据库** | PostgreSQL 15 | JSONB支持 |
| **图数据库** | Neo4j | 知识图谱 |
| **缓存** | Redis | 分布式缓存 |
| **消息队列** | RabbitMQ + Celery | 异步任务 |
| **监控** | Prometheus + Grafana | 指标监控 |
| **日志** | ELK Stack | 日志分析 |
| **部署** | Docker + K8s | 容器编排 |

---

**文档版本**：v1.0
**最后更新**：2025-12-04
